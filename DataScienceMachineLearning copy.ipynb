{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17982956-af3a-4196-a895-02c583ff3ae8",
   "metadata": {},
   "source": [
    "# [Data Science:](DataScience.ipynb) Machine Learning\n",
    "\n",
    "## Goal\n",
    "\n",
    "- *AI*: emulate intelligence\n",
    "- *machine learning:* learn from data\n",
    "- *deep learning*: learn with deep neural networks\n",
    "\n",
    "## Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0469d2c-b7fa-4b62-8caa-afc73cfe7d61",
   "metadata": {},
   "source": [
    "![](img/NN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc203fb8-b3dc-48fd-b283-64926aee20a5",
   "metadata": {},
   "source": [
    "- layers of nodes (neurons) that apply a nonlinear activation functions $f,g,\\ldots$ to a linear sum over inputs linked with weights $w_{ij}$ and constant biases $b_i$\n",
    "    - $y_i = f\\left(\\sum_j w_{ij}~g\\left(\\sum_k w_{jk}x_k+b_j\\right)+b_i\\right)$\n",
    "- originally inspired by brain, diverged, and more recently converged\n",
    "- exponential expressive power of network depth vs breadth\n",
    "- feature vector\n",
    "    - input representation of data\n",
    "- activation functions\n",
    "    - sigmoid\n",
    "        - $f(x) = 1/(1+e^{-x})$\n",
    "        - between 0 and 1, good for binary output\n",
    "    - tanh\n",
    "        - $f(x) = (e^x-e^{-x})/(e^x+e^{-x})$\n",
    "        - between -1 and 1, good for internal layers\n",
    "    - ReLU (Rectified Linear Unit)\n",
    "        - $f(x) = \\mathrm{max}(0,x)$\n",
    "        - fixes vanishing gradients, easier to compute\n",
    "    - leaky ReLU\n",
    "        - $f(x) = x$ for $x \\ge 0$ and $f(x) = \\alpha x$ for $x<0$ and small $\\alpha$\n",
    "        - fixes disappearing gradients\n",
    "- hidden layers\n",
    "    - internal layers between the inputs and outputs\n",
    "- output units\n",
    "    - linear for continuous regression\n",
    "    - sigmoidal for binary classification\n",
    "    - softmax for multiclass classification\n",
    "- loss function\n",
    "    - training goal for supervised learning\n",
    "    - types\n",
    "        - mean square error\n",
    "            - difference between points\n",
    "        - cross-entropy\n",
    "            - difference between probability distributions\n",
    "- return\n",
    "    - training goal for reinforcement learning\n",
    "    - games won, investment gain, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59738a-c9d9-45a0-888e-1c3cc9de5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-3,3,100)\n",
    "plt.plot(x,1/(1+np.exp(-x)),label='sigmoid')\n",
    "plt.plot(x,np.tanh(x),label='tanh')\n",
    "plt.plot(x,np.where(x < 0,0,x),label='ReLU')\n",
    "plt.plot(x,np.where(x < 0,0.1*x,x),'--',label='leaky ReLU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044d28f-b7c6-4bd5-8101-7337f7cb93cb",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- *back propagation*\n",
    "    - essential algorithm to propagate errors back through the network to perform the weight updates\n",
    "- *gradient descent*\n",
    "    - adjust weights to reduce loss function\n",
    "- *learning rate*\n",
    "    - the rate at which gradients are used to update weights\n",
    "- *stochastic gradient descent*\n",
    "    - for large data sets, adjust weights on random subsets of data points\n",
    "    - batch = subset size\n",
    "    - epoch = pass through entire data set\n",
    "- *momentum*\n",
    "    - add inertia to climb out of local minima\n",
    "- *ADAM (Adaptive Moment Estimation)*\n",
    "    - adjust learning rate for parameters individually\n",
    "- *L-BFGS*\n",
    "    - alternative optimizer using curvature as well as slope, which can converge faster and need less tuning\n",
    "- *early stopping*\n",
    "    - stop before loss finishes decreasing, to prevent over-fitting\n",
    "- *dropout*\n",
    "    - remove random selections of nodes during training to prevent over-fitting\n",
    "- *regularization*\n",
    "    - add penalty to control over-fitting\n",
    "    - L2 for weight norm, L1 for weight sparsity\n",
    "- *pruning*\n",
    "    - removing nodes and links with small weights\n",
    "- *quantization*\n",
    "    - reducing the bits used to represent numbers, to decrease memory and computing requirements\n",
    "- *vanishing, diverging gradients*\n",
    "    - problems in deep networks\n",
    "- *inference*\n",
    "    - using models to make predictions after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02b63c-0f7b-4ac8-9c5b-07c2a71b2505",
   "metadata": {},
   "source": [
    "## Taxonomy\n",
    "\n",
    "- *DNN*: Deep Neural Network, *MLP*: Multi-Layer Perceptron\n",
    "   - a neural network with hidden internal layers \n",
    "- *CNN*: Convolutional Neural Network\n",
    "   - a neural network that trains spatial filters to find features\n",
    "- *RNN* Recurrent Neural Network\n",
    "    - outputs are fed back to inputs, to be able to learn dynamics\n",
    "- *GAN*: Generative Adversarial Network\n",
    "   - a generator network tries to fool a discriminator network, learning how to generate data\n",
    "- *VAE*: Variational Autoencoder\n",
    "    - autoencoders connect and encoder network and a decoder network through a lower-dimensional intermediate layer\n",
    "    - they can be used to find compact representations of the data, and to syntheize data\n",
    "- *LSTM*: Long Short-Term Memory\n",
    "    - adds memory to handle long-range dependencies\n",
    "- *Transformer*\n",
    "   - adds attention to handle long-range dependencies\n",
    "- *LLM*: Large Language Model\n",
    "    - trained on a large body of text\n",
    "- *surrogate model*\n",
    "    - a model trained to emulate a more complex computation, such as a physical simulation\n",
    "    - [Physics Informed Neural Network (PINN)](https://docs.nvidia.com/physicsnemo/latest/physicsnemo-sym/user_guide/theory/phys_informed.html)\n",
    "- *[AutoML](https://automl.space/automl-tools/)*\n",
    "    - automated search over model architecture and hyperparameters\n",
    "- *Agentic AI*\n",
    "    - AI systems that can act autonomously on behalf of their users\n",
    "- *SVM*: Support Vector Machine\n",
    "    - alternative to neural networks\n",
    "    - can perform better in large dimensions, and can be easier to interpret\n",
    "    - training time is worse for large data sets, $\\sim O(N^2)$\n",
    "- *vibe coding*: programming with prompts to a LLM\n",
    "    - issues: errors, hallucination, copyright, ...\n",
    "    - need for understanding\n",
    "    - frequently needs debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2bbf4-bddd-46ba-a589-3b32bf4b4813",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- [Hugging Face](https://huggingface.co), [Kaggle](https://www.kaggle.com)\n",
    "    - huge model collections\n",
    "- [Edge Impulse](https://edgeimpulse.com), [LiteRT](https://ai.google.dev/edge/litert)\n",
    "    - models targeting edge (embedded) devices\n",
    "- [ChatGPT](https://chatgpt.com), [Claude](https://claude.ai), [Gemini](https://gemini.google.com), ...\n",
    "    - large language models that can write machine learning models\n",
    "- [ONNX](https://onnx.ai/)\n",
    "    - Open Neural Network Exchange interchange format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537a220-d7bd-477f-b77e-305f211223a5",
   "metadata": {},
   "source": [
    "## Frameworks\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/stable/index.html)\n",
    "    - easy-to-use high-level routines\n",
    "        - [classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "        - [regression](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "- [Jax](https://github.com/jax-ml/jax)\n",
    "    - lower-level control, more scalable performance\n",
    "    - [Flax](https://flax.readthedocs.io/en/stable/)\n",
    "        - simplified API for neural networks\n",
    "    - [HLO](https://openxla.org/stablehlo/tutorials/jax-export)\n",
    "        - code export from Jax\n",
    "- [PyTorch](https://pytorch.org)\n",
    "    - widely used in machine learning research\n",
    "- [TensorFlow](https://www.tensorflow.org)\n",
    "    - [TensorFlow.js](https://www.tensorflow.org/js)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e7915-a009-4da9-bc63-e3f895cb95dc",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### XOR\n",
    "- trivial but historically important example because it can't be done linearly, showing all the essential steps\n",
    "- [00,01,10,11] $\\rightarrow$ [0,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbe18b-3aa6-4460-8f66-9de936774b25",
   "metadata": {},
   "source": [
    "#### [scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4f881-b2bc-48af-bea8-31e259812f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y = [0,1,1,0]\n",
    "classifier = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(4),activation='tanh',random_state=1)\n",
    "classifier.fit(X,y)\n",
    "print(f\"score: {classifier.score(X,y)}\")\n",
    "print(\"Predictions:\")\n",
    "np.c_[X,classifier.predict(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238c114-a6a2-463f-8725-c99efa968a11",
   "metadata": {},
   "source": [
    "### Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68205e1a-0db2-466f-8c4b-89c31ae6ec47",
   "metadata": {},
   "source": [
    "### import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random,grad,jit\n",
    "#\n",
    "# init random key\n",
    "#\n",
    "key = random.PRNGKey(0)\n",
    "#\n",
    "# XOR training data\n",
    "#\n",
    "X = jnp.array([[0,0],[0,1],[1,0],[1,1]],dtype=jnp.int8)\n",
    "y = jnp.array([0,1,1,0],dtype=jnp.int8).reshape(4,1)\n",
    "#\n",
    "# forward pass\n",
    "#\n",
    "@jit\n",
    "def forward(params,layer_0):\n",
    "    Weight1,bias1,Weight2,bias2 = params\n",
    "    layer_1 = jnp.tanh(layer_0@Weight1+bias1)\n",
    "    layer_2 = jax.nn.sigmoid(layer_1@Weight2+bias2)\n",
    "    return layer_2\n",
    "#\n",
    "# loss function\n",
    "#\n",
    "@jit\n",
    "def loss(params):\n",
    "    ypred = forward(params,X)\n",
    "    return jnp.mean((ypred-y)**2)\n",
    "#\n",
    "# gradient update step\n",
    "#\n",
    "@jit\n",
    "def update(params,rate=0.5):\n",
    "    gradient = grad(loss)(params)\n",
    "    return jax.tree.map(lambda params,gradient:params-rate*gradient,params,gradient)\n",
    "#\n",
    "# parameter initialization\n",
    "#\n",
    "def init_params(key):\n",
    "    key1,key2 = random.split(key)\n",
    "    Weight1 = 0.5*random.normal(key1,(2,4))\n",
    "    bias1 = jnp.zeros(4)\n",
    "    Weight2 = 0^.5*random.normal(key2,(4,1))\n",
    "    bias2 = jnp.zeros(1)\n",
    "    return (Weight1,bias1,Weight2,bias2)\n",
    "#\n",
    "# initialize parameters\n",
    "#\n",
    "params = init_params(key)\n",
    "#\n",
    "# training steps\n",
    "#\n",
    "for step in range(201):\n",
    "    params = update(params,rate=10)\n",
    "    if step%100 == 0:\n",
    "        print(f\"step {step:4d} loss={loss(params):.4f}\")\n",
    "#\n",
    "# evaluate fit\n",
    "#\n",
    "pred = forward(params,X)\n",
    "jnp.set_printoptions(precision=2)\n",
    "print(\"\\nPredictions:\")\n",
    "print(jnp.c_[X,pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1bfa0-9b6a-4375-957c-e0da2b32870c",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "- historically important non-trivial example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371fbbc-40c7-49e9-9b5f-52ce5d6bcb04",
   "metadata": {},
   "source": [
    "#### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa5542-77e7-4c93-b80a-dbbd5c8707f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "xtrain = np.load('datasets/MNIST/xtrain.npy')\n",
    "ytrain = np.load('datasets/MNIST/ytrain.npy')\n",
    "xtest = np.load('datasets/MNIST/xtest.npy')\n",
    "ytest = np.load('datasets/MNIST/ytest.npy')\n",
    "print(f\"read {xtrain.shape[1]} byte data records, {xtrain.shape[0]} training examples, {xtest.shape[0]} testing examples\\n\")\n",
    "classifier = MLPClassifier(solver='adam',hidden_layer_sizes=(100),activation='relu',random_state=1,verbose=True,tol=0.05)\n",
    "classifier.fit(xtrain,ytrain)\n",
    "print(f\"\\ntest score: {classifier.score(xtest,ytest)}\\n\")\n",
    "predictions = classifier.predict(xtest)\n",
    "fig,axs = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    axs[i].imshow(jnp.reshape(xtest[i],(28,28)))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"predict: {predictions[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89442436-e434-495b-9b29-c44d1fae4a15",
   "metadata": {},
   "source": [
    "#### Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc55be-b770-4319-a83c-ac7ad2bbe2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random,grad,jit\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "# hyperparameters\n",
    "#\n",
    "data_size = 28*28\n",
    "hidden_size = data_size//10\n",
    "output_size = 10\n",
    "batch_size = 5000\n",
    "train_steps = 25\n",
    "learning_rate = 0.5\n",
    "#\n",
    "# init random key\n",
    "#\n",
    "key = random.PRNGKey(0)\n",
    "#\n",
    "# load MNIST data\n",
    "#\n",
    "xtrain = jnp.load('datasets/MNIST/xtrain.npy')\n",
    "ytrain = jnp.load('datasets/MNIST/ytrain.npy')\n",
    "xtest = jnp.load('datasets/MNIST/xtest.npy')\n",
    "ytest = jnp.load('datasets/MNIST/ytest.npy')\n",
    "print(f\"read {xtrain.shape[1]} byte data records, {xtrain.shape[0]} training examples, {xtest.shape[0]} testing examples\\n\")\n",
    "#\n",
    "# forward pass\n",
    "#\n",
    "@jit\n",
    "def forward(params,layer_0):\n",
    "    Weight1,bias1,Weight2,bias2 = params\n",
    "    layer_1 = jnp.tanh(layer_0@Weight1+bias1)\n",
    "    layer_2 = layer_1@Weight2+bias2\n",
    "    return layer_2\n",
    "#\n",
    "# loss function\n",
    "#\n",
    "@jit\n",
    "def loss(params,xtrain,ytrain):\n",
    "    logits = forward(params,xtrain)\n",
    "    probs = jnp.exp(logits)/jnp.sum(jnp.exp(logits),axis=1,keepdims=True)\n",
    "    error = 1-jnp.mean(probs[jnp.arange(len(ytrain)),ytrain])\n",
    "    return error\n",
    "#\n",
    "# gradient update step\n",
    "#\n",
    "@jit\n",
    "def update(params,xtrain,ytrain,rate):\n",
    "    gradient = grad(loss)(params,xtrain,ytrain)\n",
    "    return jax.tree.map(lambda params,gradient:params-rate*gradient,params,gradient)\n",
    "#\n",
    "# parameter initialization\n",
    "#\n",
    "def init_params(key,xsize,hidden,output):\n",
    "    key1,key = random.split(key)\n",
    "    Weight1 = 0.01*random.normal(key1,(xsize,hidden))\n",
    "    bias1 = jnp.zeros(hidden)\n",
    "    key2,key = random.split(key)\n",
    "    Weight2 = 0.01*random.normal(key2,(hidden,output))\n",
    "    bias2 = jnp.zeros(output)\n",
    "    return (Weight1,bias1,Weight2,bias2)\n",
    "#\n",
    "# initialize parameters\n",
    "#\n",
    "params = init_params(key,data_size,hidden_size,output_size)\n",
    "#\n",
    "# train\n",
    "#\n",
    "print(f\"starting loss: {loss(params,xtrain,ytrain):.3f}\\n\")\n",
    "for batch in range(0,len(ytrain),batch_size):\n",
    "    xbatch = xtrain[batch:batch+batch_size]\n",
    "    ybatch = ytrain[batch:batch+batch_size]\n",
    "    print(f\"batch {batch}: \",end='')\n",
    "    for step in range(train_steps):\n",
    "        params = update(params,xbatch,ybatch,rate=learning_rate)\n",
    "    print(f\"loss {loss(params,xbatch,ybatch):.3f}\")\n",
    "#\n",
    "# test\n",
    "#\n",
    "logits = forward(params,xtest)\n",
    "probs = jnp.exp(logits)/jnp.sum(jnp.exp(logits),axis=1,keepdims=True)\n",
    "error = 1-jnp.mean(probs[jnp.arange(len(ytest)),ytest])\n",
    "print(f\"\\ntest loss: {error:.3f}\\n\")\n",
    "#\n",
    "# plot\n",
    "#\n",
    "fig,axs = plt.subplopts(1,5)\n",
    "for i in range(5):\n",
    "    axs[i].imshow(jnp.reshape(xtest[i],(28,28)))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"predict: {jnp.argmax(probs[i])}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc83e62-87bc-4756-9cc9-24f5ebc4261b",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "- Fit a machine learning model to your data\n",
    "\n",
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc030d4c-3d59-471b-9e21-22beeb108071",
   "metadata": {},
   "source": [
    "Note: write here a trained ai that can identify an object by seeing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6faa6f8-c5f1-4832-857c-de4599125777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53303 files belonging to 19 classes.\n",
      "Using 42643 files for training.\n",
      "Found 53303 files belonging to 19 classes.\n",
      "Using 10660 files for validation.\n",
      "Classes: ['buffalo', 'capybara', 'cat', 'cow', 'deer', 'dog', 'elephant', 'flamingo', 'giraffe', 'jaguar', 'kangaroo', 'lion', 'parrot', 'penguin', 'rhino', 'sheep', 'tiger', 'turtle', 'zebra']\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 22:30:40.750065: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 969/1333\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 109ms/step - accuracy: 0.2592 - loss: 2.4949"
     ]
    }
   ],
   "source": [
    "### import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "DATASET_PATH = \"animal_dataset\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(len(class_names), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Performance\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcc2e6-fe3d-46db-bf9f-45bab5ff16ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee8e5a-229f-425b-8990-fd63c08ea8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
